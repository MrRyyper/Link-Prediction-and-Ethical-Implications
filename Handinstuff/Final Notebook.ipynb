{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d69dafd5-bd01-4187-b84b-3e1296fb1272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded\n"
     ]
    }
   ],
   "source": [
    "# ===== IMPORTS =====\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import combinations, product\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "# Define random seed\n",
    "seed = 42\n",
    "\n",
    "# Individual Feature Configuration - Set to True/False to enable/disable each feature\n",
    "FEATURE_CONFIG = {\n",
    "    # === ATTRIBUTE FEATURES ===\n",
    "    'attribute_i': True,              # Individual attribute value for node i\n",
    "    'attribute_j': True,              # Individual attribute value for node j\n",
    "    'attribute_equality': True,       # Attribute equality indicator\n",
    "    \n",
    "    # === BASIC NODE FEATURES ===\n",
    "    'degree_i': True,                 # Degree of node i\n",
    "    'degree_j': True,                 # Degree of node j\n",
    "    'clustering_i': True,             # Clustering coefficient of node i\n",
    "    'clustering_j': True,             # Clustering coefficient of node j\n",
    "    'pagerank_i': True,               # PageRank of node i\n",
    "    'pagerank_j': True,               # PageRank of node j\n",
    "    \n",
    "    # === NEIGHBORHOOD FEATURES ===\n",
    "    'common_neighbors': True,         # Number of common neighbors\n",
    "    'jaccard_coefficient': True,      # Jaccard coefficient\n",
    "    'adamic_adar': True,              # Adamic-Adar index\n",
    "    'resource_allocation': True,       # Resource Allocation index\n",
    "    'preferential_attachment': True,   # Preferential Attachment\n",
    "    'salton_index': False,             # Salton index\n",
    "    'sorensen_index': False,           # Sorensen index\n",
    "    'two_hop_neighbors': False,        # 2-hop neighbors count\n",
    "    \n",
    "    # === DERIVED FEATURES ===\n",
    "    'degree_sum': True,               # Sum of degrees\n",
    "    'degree_diff': True,              # Absolute difference of degrees\n",
    "    'degree_product': True,            # Product of degrees\n",
    "    'degree_ratio': True,             # Ratio of degrees (min/max)\n",
    "    'clustering_avg': True,           # Average clustering coefficient\n",
    "    'pagerank_avg': True,             # Average PageRank\n",
    "    \n",
    "    # === COMMUNITY FEATURES ===\n",
    "    'same_community': True,           # Same community indicator\n",
    "    'community_size_i': True,         # Community size of node i\n",
    "    'community_size_j': True,         # Community size of node j\n",
    "    'community_size_diff': True,      # Absolute difference in community sizes\n",
    "    'community_size_ratio': True,     # Ratio of community sizes\n",
    "}\n",
    "\n",
    "# Model Configuration\n",
    "MODEL_CONFIG = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 3,\n",
    "    'min_samples_split': 10,\n",
    "    'min_samples_leaf': 3,\n",
    "    'random_state': seed,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "# Cross-validation Configuration\n",
    "CV_CONFIG = {\n",
    "    'n_splits': 5,\n",
    "    'shuffle': True,\n",
    "    'random_state': seed\n",
    "}\n",
    "\n",
    "# Community Configuration\n",
    "COMMUNITY_CONFIG = {\n",
    "    'resolution': 1.25,  # Can be changed for different resolutions\n",
    "    'test_resolutions': np.arange(0.5, 2.0, 0.1).tolist()  # For testing multiple resolutions\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a296252b-8afd-49f3-bae4-72b6500a98da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph loaded with 1500 nodes and 6600 edges\n",
      "   ✓ Encoded 'attribute': ['d', 'f', 'l', 'm', 'x', 'y']\n",
      "Data loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# ===== DATA LOADING =====\n",
    "def load_data():\n",
    "    \"\"\"Load graph and attributes data\"\"\"\n",
    "    # Load graph\n",
    "    path = \"./../assignment2_files_2025/edges_train.edgelist\"\n",
    "    G = nx.read_edgelist(path, delimiter=',', nodetype=int, create_using=nx.Graph())\n",
    "    print(f\"Graph loaded with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n",
    "    \n",
    "    # Load attributes\n",
    "    pathattributes = \"./../assignment2_files_2025/attributes.csv\"\n",
    "    attributes_df = pd.read_csv(pathattributes)\n",
    "    node_id_col = attributes_df.columns[0]\n",
    "    attribute_cols = [col for col in attributes_df.columns if col != node_id_col]\n",
    "    \n",
    "    # Encode categorical to numerical\n",
    "    for col in attribute_cols:\n",
    "        if attributes_df[col].dtype == 'object':\n",
    "            le = LabelEncoder()\n",
    "            attributes_df[col] = le.fit_transform(attributes_df[col].astype(str))\n",
    "            print(f\"   ✓ Encoded '{col}': {list(le.classes_)}\")\n",
    "    \n",
    "    attributes_dict = attributes_df.set_index(node_id_col).to_dict('index')\n",
    "    \n",
    "    return G, attributes_dict, attribute_cols\n",
    "\n",
    "# Load data\n",
    "G, attributes_dict, attribute_cols = load_data()\n",
    "print(\"Data loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc71bac4-a4f0-4ef0-ab55-4c9bf1a9d195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering function defined\n",
      "Precomputed metrics helper function defined\n"
     ]
    }
   ],
   "source": [
    "# ===== FEATURE ENGINEERING =====\n",
    "def get_features(G, i, j, feature_config=FEATURE_CONFIG, precomputed_metrics=None):\n",
    "    \"\"\"\n",
    "    Features voor node pair (i, j) based on configuration\n",
    "    Args:\n",
    "        G: NetworkX graph\n",
    "        i, j: Node pair\n",
    "        feature_config: Feature configuration dictionary\n",
    "        precomputed_metrics: Dictionary with precomputed metrics (pa, pagerank, clustering, community_dict, comm_sizes)\n",
    "    Returns: (feature_array, feature_names)\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    feature_names = []\n",
    "    \n",
    "    # Get attribute values\n",
    "    attrs_i = attributes_dict.get(i, {})\n",
    "    attrs_j = attributes_dict.get(j, {})\n",
    "    \n",
    "    # --- Attribute features ---\n",
    "    for col in attribute_cols:\n",
    "        val_i = attrs_i.get(col, 0)\n",
    "        val_j = attrs_j.get(col, 0)\n",
    "        \n",
    "        if feature_config['attribute_i']:\n",
    "            features.append(val_i)\n",
    "            feature_names.append(f\"{col}_i\")\n",
    "        \n",
    "        if feature_config['attribute_j']:\n",
    "            features.append(val_j)\n",
    "            feature_names.append(f\"{col}_j\")\n",
    "        \n",
    "        if feature_config['attribute_equality']:\n",
    "            features.append(int(val_i == val_j))\n",
    "            feature_names.append(f\"{col}_eq\")\n",
    "    \n",
    "    # --- Basic node features ---\n",
    "    deg_i = G.degree(i)\n",
    "    deg_j = G.degree(j)\n",
    "    \n",
    "    # Use precomputed metrics if available, otherwise compute on-the-fly\n",
    "    if precomputed_metrics:\n",
    "        cc_i = precomputed_metrics['clustering'].get(i, 0)\n",
    "        cc_j = precomputed_metrics['clustering'].get(j, 0)\n",
    "        pr_i = precomputed_metrics['pagerank'].get(i, 0)\n",
    "        pr_j = precomputed_metrics['pagerank'].get(j, 0)\n",
    "    else:\n",
    "        # Fallback: compute on-the-fly (not recommended for performance)\n",
    "        cc_i = nx.clustering(G, i)\n",
    "        cc_j = nx.clustering(G, j)\n",
    "        pr_i = nx.pagerank(G)[i] if i in nx.pagerank(G) else 0\n",
    "        pr_j = nx.pagerank(G)[j] if j in nx.pagerank(G) else 0\n",
    "    \n",
    "    if feature_config['degree_i']:\n",
    "        features.append(deg_i)\n",
    "        feature_names.append('deg_i')\n",
    "    \n",
    "    if feature_config['degree_j']:\n",
    "        features.append(deg_j)\n",
    "        feature_names.append('deg_j')\n",
    "    \n",
    "    if feature_config['clustering_i']:\n",
    "        features.append(cc_i)\n",
    "        feature_names.append('cc_i')\n",
    "    \n",
    "    if feature_config['clustering_j']:\n",
    "        features.append(cc_j)\n",
    "        feature_names.append('cc_j')\n",
    "    \n",
    "    if feature_config['pagerank_i']:\n",
    "        features.append(pr_i)\n",
    "        feature_names.append('pr_i')\n",
    "    \n",
    "    if feature_config['pagerank_j']:\n",
    "        features.append(pr_j)\n",
    "        feature_names.append('pr_j')\n",
    "    \n",
    "    # --- Neighborhood features ---\n",
    "    common = list(nx.common_neighbors(G, i, j))\n",
    "    cn_ij = len(common)\n",
    "    \n",
    "    if feature_config['common_neighbors']:\n",
    "        features.append(cn_ij)\n",
    "        feature_names.append('cn_ij')\n",
    "    \n",
    "    if feature_config['jaccard_coefficient']:\n",
    "        jc_ij = next(nx.jaccard_coefficient(G, [(i,j)]))[2]\n",
    "        features.append(jc_ij)\n",
    "        feature_names.append('jc_ij')\n",
    "    \n",
    "    if feature_config['adamic_adar']:\n",
    "        aa_ij = sum(1.0 / np.log(G.degree(z)) for z in common if G.degree(z) > 1)\n",
    "        features.append(aa_ij)\n",
    "        feature_names.append('aa_ij')\n",
    "    \n",
    "    if feature_config['resource_allocation']:\n",
    "        ra_ij = sum(1.0 / G.degree(z) for z in common if G.degree(z) > 0)\n",
    "        features.append(ra_ij)\n",
    "        feature_names.append('ra_ij')\n",
    "    \n",
    "    if feature_config['preferential_attachment']:\n",
    "        if precomputed_metrics and 'pa' in precomputed_metrics:\n",
    "            pa_ij = precomputed_metrics['pa'][i, j]\n",
    "        else:\n",
    "            # Fallback: compute on-the-fly (very expensive!)\n",
    "            pa_ij = next(nx.preferential_attachment(G, [(i, j)]))[2]\n",
    "        features.append(pa_ij)\n",
    "        feature_names.append('pa_ij')\n",
    "    \n",
    "    if feature_config['salton_index']:\n",
    "        salton = cn_ij / np.sqrt(deg_i * deg_j) if (deg_i * deg_j) > 0 else 0\n",
    "        features.append(salton)\n",
    "        feature_names.append('salton')\n",
    "    \n",
    "    if feature_config['sorensen_index']:\n",
    "        sorensen = (2 * cn_ij) / (deg_i + deg_j) if (deg_i + deg_j) > 0 else 0\n",
    "        features.append(sorensen)\n",
    "        feature_names.append('sorensen')\n",
    "    \n",
    "    if feature_config['two_hop_neighbors']:\n",
    "        # 2 hop neighbors\n",
    "        dist2_i = set(nx.single_source_shortest_path_length(G, i, cutoff=2))\n",
    "        dist2_j = set(nx.single_source_shortest_path_length(G, j, cutoff=2))\n",
    "        \n",
    "        # remove the nodes themselves and their direct neighbors\n",
    "        dist2_i -= {i} | set(G.neighbors(i))\n",
    "        dist2_j -= {j} | set(G.neighbors(j))\n",
    "        \n",
    "        # intersection of these \"exactly distance-2\" neighborhoods\n",
    "        dist2_count = len(dist2_i & dist2_j)\n",
    "        features.append(dist2_count)\n",
    "        feature_names.append('dist2_count')\n",
    "    \n",
    "    # --- Derived features ---\n",
    "    if feature_config['degree_sum']:\n",
    "        deg_sum = deg_i + deg_j\n",
    "        features.append(deg_sum)\n",
    "        feature_names.append('deg_sum')\n",
    "    \n",
    "    if feature_config['degree_diff']:\n",
    "        deg_diff = abs(deg_i - deg_j)\n",
    "        features.append(deg_diff)\n",
    "        feature_names.append('deg_diff')\n",
    "    \n",
    "    if feature_config['degree_product']:\n",
    "        deg_product = deg_i * deg_j\n",
    "        features.append(deg_product)\n",
    "        feature_names.append('deg_product')\n",
    "    \n",
    "    if feature_config['degree_ratio']:\n",
    "        deg_ratio = min(deg_i, deg_j) / max(deg_i, deg_j) if max(deg_i, deg_j) > 0 else 0\n",
    "        features.append(deg_ratio)\n",
    "        feature_names.append('deg_ratio')\n",
    "    \n",
    "    if feature_config['clustering_avg']:\n",
    "        cc_avg = (cc_i + cc_j) / 2\n",
    "        features.append(cc_avg)\n",
    "        feature_names.append('cc_avg')\n",
    "    \n",
    "    if feature_config['pagerank_avg']:\n",
    "        pr_avg = (pr_i + pr_j) / 2\n",
    "        features.append(pr_avg)\n",
    "        feature_names.append('pr_avg')\n",
    "    \n",
    "    # --- Community features ---\n",
    "    if precomputed_metrics and 'community_dict' in precomputed_metrics:\n",
    "        comm_i = precomputed_metrics['community_dict'].get(i, -1)\n",
    "        comm_j = precomputed_metrics['community_dict'].get(j, -1)\n",
    "        comm_sizes = precomputed_metrics['comm_sizes']\n",
    "    else:\n",
    "        # Fallback: compute communities on-the-fly (expensive!)\n",
    "        communities = list(nx.algorithms.community.greedy_modularity_communities(G))\n",
    "        community_dict = {n: cid for cid, nodes in enumerate(communities) for n in nodes}\n",
    "        comm_sizes = {cid: len(nodes) for cid, nodes in enumerate(communities)}\n",
    "        comm_i = community_dict.get(i, -1)\n",
    "        comm_j = community_dict.get(j, -1)\n",
    "    \n",
    "    if feature_config['same_community']:\n",
    "        same_comm = int(comm_i == comm_j)\n",
    "        features.append(same_comm)\n",
    "        feature_names.append('same_comm')\n",
    "    \n",
    "    if feature_config['community_size_i']:\n",
    "        size_i = comm_sizes.get(comm_i, 0)\n",
    "        features.append(size_i)\n",
    "        feature_names.append('size_i')\n",
    "    \n",
    "    if feature_config['community_size_j']:\n",
    "        size_j = comm_sizes.get(comm_j, 0)\n",
    "        features.append(size_j)\n",
    "        feature_names.append('size_j')\n",
    "    \n",
    "    if feature_config['community_size_diff']:\n",
    "        size_i = comm_sizes.get(comm_i, 0)\n",
    "        size_j = comm_sizes.get(comm_j, 0)\n",
    "        abs_size_diff = abs(size_i - size_j)\n",
    "        features.append(abs_size_diff)\n",
    "        feature_names.append('abs_size_diff')\n",
    "    \n",
    "    if feature_config['community_size_ratio']:\n",
    "        size_i = comm_sizes.get(comm_i, 0)\n",
    "        size_j = comm_sizes.get(comm_j, 0)\n",
    "        if size_i + size_j > 0:\n",
    "            rel_size_ratio = size_i / (size_j + 1e-6)\n",
    "        else:\n",
    "            rel_size_ratio = 0\n",
    "        features.append(rel_size_ratio)\n",
    "        feature_names.append('rel_size_ratio')\n",
    "    \n",
    "    return np.array(features, dtype=float), feature_names\n",
    "\n",
    "def precompute_metrics(G_train, community_resolution=1.0):\n",
    "    \"\"\"\n",
    "    Pre-compute all expensive graph metrics using ONLY training graph\n",
    "    Args:\n",
    "        G_train: NetworkX training graph (only training edges)\n",
    "        community_resolution: Resolution parameter for community detection\n",
    "    Returns:\n",
    "        Dictionary with precomputed metrics\n",
    "    \"\"\"\n",
    "    N = G_train.number_of_nodes()\n",
    "    \n",
    "    # Preferential Attachment\n",
    "    pa = np.zeros((N, N))\n",
    "    for u, v, p in nx.preferential_attachment(G_train, [(i, j) for i in range(N) for j in range(N)]):\n",
    "        pa[u, v] = p\n",
    "    \n",
    "    # PageRank\n",
    "    pagerank = nx.pagerank(G_train)\n",
    "    \n",
    "    # Clustering        \n",
    "    clustering = nx.clustering(G_train)\n",
    "    \n",
    "    # Communities\n",
    "    communities = list(nx.algorithms.community.greedy_modularity_communities(G_train, resolution=community_resolution))\n",
    "    community_dict = {n: cid for cid, nodes in enumerate(communities) for n in nodes}\n",
    "    comm_sizes = {cid: len(nodes) for cid, nodes in enumerate(communities)}\n",
    "    \n",
    "    return {\n",
    "        'pa': pa,\n",
    "        'pagerank': pagerank,\n",
    "        'clustering': clustering,\n",
    "        'community_dict': community_dict,\n",
    "        'comm_sizes': comm_sizes\n",
    "    }\n",
    "\n",
    "print(\"Feature engineering function defined\")\n",
    "print(\"Precomputed metrics helper function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e469841b-a33c-4827-9ba3-7b82ffc8b11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single configuration testing function defined\n"
     ]
    }
   ],
   "source": [
    "# ===== SINGLE CONFIGURATION TESTING =====\n",
    "def test_single_configuration(feature_config, model_config=MODEL_CONFIG, cv_config=CV_CONFIG, community_resolution=1.25):\n",
    "    \"\"\"Test a single feature configuration\"\"\"\n",
    "    print(f\"\\n=== Testing Configuration ===\")\n",
    "    print(f\"Enabled features: {[k for k, v in feature_config.items() if v]}\")\n",
    "    \n",
    "    # Initialize results\n",
    "    auc_scores = []\n",
    "    acc_scores = []\n",
    "    \n",
    "    # K-fold cross validation\n",
    "    edges = np.array(list(G.edges()))\n",
    "    kf = KFold(n_splits=cv_config['n_splits'], shuffle=cv_config['shuffle'], random_state=cv_config['random_state'])\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(edges), 1):\n",
    "        print(f\"\\n--- Fold {fold} ---\")\n",
    "        \n",
    "        # Split edges\n",
    "        train_edges = edges[train_idx]\n",
    "        val_edges = edges[val_idx]\n",
    "        \n",
    "        # Build training graph\n",
    "        G_train = nx.Graph()\n",
    "        G_train.add_nodes_from(G.nodes())\n",
    "        G_train.add_edges_from(train_edges)\n",
    "        \n",
    "        # Pre-compute metrics using helper function\n",
    "        precomputed_metrics = precompute_metrics(G_train, community_resolution)\n",
    "        \n",
    "        # Build training data\n",
    "        pos_train = train_edges\n",
    "        rng = np.random.default_rng(seed)\n",
    "        non_edges = np.array(list(nx.non_edges(G_train)))\n",
    "        neg_train = non_edges[rng.choice(len(non_edges), size=len(pos_train), replace=False)]\n",
    "        \n",
    "        X_train_features = []\n",
    "        for (u, v) in np.vstack([pos_train, neg_train]):\n",
    "            feat, _ = get_features(G_train, u, v, feature_config, precomputed_metrics)\n",
    "            X_train_features.append(feat)\n",
    "        \n",
    "        y_train = [1]*len(pos_train) + [0]*len(neg_train)\n",
    "    \n",
    "        # Build validation data\n",
    "        pos_val = val_edges\n",
    "        rng2 = np.random.default_rng(seed+1)\n",
    "        non_edges = np.array(list(nx.non_edges(G_train)))\n",
    "        neg_val = non_edges[rng2.choice(len(non_edges), size=len(pos_val), replace=False)]\n",
    "        \n",
    "        X_val_features = []\n",
    "        for (u, v) in np.vstack([pos_val, neg_val]):\n",
    "            feat, _ = get_features(G_train, u, v, feature_config, precomputed_metrics)\n",
    "            X_val_features.append(feat)\n",
    "        \n",
    "        y_val = [1]*len(pos_val) + [0]*len(neg_val)\n",
    "    \n",
    "        # Train model\n",
    "        clf = RandomForestClassifier(**model_config)\n",
    "        clf.fit(X_train_features, y_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred_proba = clf.predict_proba(X_val_features)[:,1]\n",
    "        auc = roc_auc_score(y_val, y_pred_proba)\n",
    "        auc_scores.append(auc)\n",
    "        \n",
    "        y_pred_val = clf.predict(X_val_features)\n",
    "        val_acc = accuracy_score(y_val, y_pred_val)\n",
    "        acc_scores.append(val_acc)\n",
    "        \n",
    "        print(f\"Fold {fold} AUC: {auc:.4f}, Accuracy: {val_acc:.4f}\")\n",
    "    \n",
    "    # Calculate final results\n",
    "    mean_auc = np.mean(auc_scores)\n",
    "    std_auc = np.std(auc_scores)\n",
    "    mean_acc = np.mean(acc_scores)\n",
    "    std_acc = np.std(acc_scores)\n",
    "    \n",
    "    print(f\"\\n=== Results ===\")\n",
    "    print(f\"Mean AUC: {mean_auc:.4f} ± {std_auc:.4f}\")\n",
    "    print(f\"Mean Accuracy: {mean_acc:.4f} ± {std_acc:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'mean_auc': mean_auc,\n",
    "        'std_auc': std_auc,\n",
    "        'mean_acc': mean_acc,\n",
    "        'std_acc': std_acc,\n",
    "        'auc_scores': auc_scores,\n",
    "        'acc_scores': acc_scores\n",
    "    }\n",
    "\n",
    "print(\"Single configuration testing function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "319ab44a-f002-4567-b85e-78c793ae2ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test current configuration\n",
    "current_result = test_single_configuration(FEATURE_CONFIG)\n",
    "print(f\"\\nCurrent configuration performance:\")\n",
    "print(f\"AUC: {current_result['mean_auc']:.4f} ± {current_result['std_auc']:.4f}\")\n",
    "print(f\"Accuracy: {current_result['mean_acc']:.4f} ± {current_result['std_acc']:.4f}\")\n",
    "\n",
    "clf = current_result['clf']\n",
    "\n",
    "importances = clf.feature_importances_\n",
    "\n",
    "# Get feature names\n",
    "feature_names = [k for k, v in FEATURE_CONFIG.items() if v is True]\n",
    "\n",
    "# Rank features\n",
    "labels = [feature_name.replace(\"_\", \" \") for feature_name in feature_names]\n",
    "feat_df = pd.DataFrame({\n",
    "    'Feature': labels,\n",
    "    'Importance': importances\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "# ===== Feature Importance =====\n",
    "# Plot\n",
    "plt.figure(1)\n",
    "plt.barh(feat_df['Feature'][:20][::-1], feat_df['Importance'][:20][::-1])\n",
    "plt.title(\"Top 20 Feature Importances (Random Forest)\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.show()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"importances.jpg\")\n",
    "\n",
    "# X = np.array([get_features(G, u, v) for (u,v) in np.vstack([pos_train, neg_train])])\n",
    "X = current_result['features']\n",
    "corr = abs(pd.DataFrame(X, columns=feature_names).corr())\n",
    "plt.figure(2)\n",
    "plt.imshow(corr, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.xticks(np.arange(len(feature_names)), labels, rotation=45, size = 'x-small')\n",
    "plt.yticks(np.arange(len(feature_names)), labels, size = 'x-small')\n",
    "plt.colorbar()\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.subplots_adjust(bottom=0.3, left=0.3)  # space for labels\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ===== Correlation plotting =====\n",
    "# --- Data setup ---\n",
    "X = current_result['features']\n",
    "corr = pd.DataFrame(X, columns=feature_names).corr().abs()\n",
    "\n",
    "# --- Order features by their average absolute correlation ---\n",
    "mean_corr = corr.mean().sort_values(ascending=False)\n",
    "ordered_features = mean_corr.index\n",
    "corr_sorted = corr.loc[ordered_features, ordered_features]\n",
    "ordered_features_labels = [feature_name.replace(\"_\", \" \") for feature_name in ordered_features]\n",
    "\n",
    "# --- Plot ---\n",
    "plt.figure(figsize=(12, 10))\n",
    "im = plt.imshow(corr_sorted, cmap='Blues', vmin=0, vmax=1)\n",
    "\n",
    "# --- Tick labels ---\n",
    "plt.xticks(\n",
    "    ticks=np.arange(len(ordered_features)),\n",
    "    labels=ordered_features_labels,\n",
    "    rotation=90,\n",
    "    fontsize=9\n",
    ")\n",
    "plt.yticks(\n",
    "    ticks=np.arange(len(ordered_features)),\n",
    "    labels=ordered_features_labels,\n",
    "    fontsize=9\n",
    ")\n",
    "\n",
    "# --- Colorbar ---\n",
    "cbar = plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "cbar.set_label('Absolute Correlation', rotation=270, labelpad=15)\n",
    "\n",
    "# --- Titles and layout ---\n",
    "plt.title(\"Feature Correlation Heatmap (Ordered by Average Correlation)\", fontsize=13, pad=20)\n",
    "plt.grid(False)\n",
    "\n",
    "# --- Improve layout spacing ---\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(bottom=0.25, left=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43902e0d-a1fb-42a4-9681-0abb2b1e8b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== INDIVIDUAL FEATURE TESTING =====\n",
    "def test_individual_features(max_features=8):\n",
    "    \"\"\"Test individual features to identify the most important ones\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Test each individual feature\n",
    "    print(\"\\n=== Testing Individual Features ===\")\n",
    "    for feature_name in FEATURE_CONFIG.keys():\n",
    "        config = {key: False for key in FEATURE_CONFIG.keys()}\n",
    "        config[feature_name] = True\n",
    "        \n",
    "        print(f\"\\nTesting {feature_name}...\")\n",
    "        result = test_single_configuration(config)\n",
    "        result['config_name'] = feature_name\n",
    "        result['features'] = [feature_name]\n",
    "        results.append(result)\n",
    "    \n",
    "    # Sort by AUC to find best individual features\n",
    "    results.sort(key=lambda x: x['mean_auc'], reverse=True)\n",
    "    \n",
    "    # Test combinations of top features\n",
    "    print(\"\\n=== Testing Top Feature Combinations ===\")\n",
    "    top_features = [r['config_name'] for r in results[:max_features]]\n",
    "    \n",
    "    # Test all combinations of 2 top features\n",
    "    for combo in combinations(top_features, 2):\n",
    "        config = {key: False for key in FEATURE_CONFIG.keys()}\n",
    "        config_name = f\"{combo[0]} + {combo[1]}\"\n",
    "        \n",
    "        for feature in combo:\n",
    "            config[feature] = True\n",
    "        \n",
    "        print(f\"\\nTesting {config_name}...\")\n",
    "        result = test_single_configuration(config)\n",
    "        result['config_name'] = config_name\n",
    "        result['features'] = list(combo)\n",
    "        results.append(result)\n",
    "    \n",
    "    # Test all features\n",
    "    print(\"\\n=== Testing All Features ===\")\n",
    "    config = {key: True for key in FEATURE_CONFIG.keys()}\n",
    "    result = test_single_configuration(config)\n",
    "    result['config_name'] = 'all_features'\n",
    "    result['features'] = list(FEATURE_CONFIG.keys())\n",
    "    results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "results = test_individual_features(max_features=8)\n",
    "\n",
    "import pickle\n",
    "with open(\"results.pkl\", \"wb\") as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34317043-8b35-4254-a309-bb7ef3405262",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Plotting single and multi feature results\n",
    "# Filter entries with a single feature\n",
    "single_feature_results = [r for r in results if len(r['features']) == 1]\n",
    "\n",
    "# Sort by mean accuracy (descending)\n",
    "sorted_results = sorted(single_feature_results, key=lambda x: x['mean_acc'], reverse=True)\n",
    "\n",
    "# Keep only top 10\n",
    "top_results = sorted_results[:20]\n",
    "\n",
    "# Extract data\n",
    "features = [r['features'][0] for r in top_results]\n",
    "mean_accs = [r['mean_acc'] for r in top_results]\n",
    "mean_aucs = [r['mean_auc'] for r in top_results]\n",
    "\n",
    "# Y positions\n",
    "y = np.arange(len(features))\n",
    "height = 0.4  # bar thickness\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "\n",
    "# AUC bars (behind, red)\n",
    "plt.barh(y, mean_aucs, height=height, color='red', alpha=0.5, label='Mean AUC')\n",
    "\n",
    "# Accuracy bars (front, blue)\n",
    "plt.barh(y, mean_accs, height=height*0.8, color='blue', alpha=0.5, label='Mean Accuracy')\n",
    "\n",
    "# Labels and formatting\n",
    "plt.title(\"Top 20 Features by Accuracy\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Score\", fontsize=12)\n",
    "plt.ylabel(\"Feature\", fontsize=12)\n",
    "plt.yticks(y, features)\n",
    "plt.xlim(0, 1)\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Reverse order so best at top\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# Legend outside the plot\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1.02, 1), borderaxespad=0.)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711be980-d734-4d50-8d24-0eefc2df0482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Resolution =====\n",
    "#%% loopsearch communities\n",
    "# Community Configuration\n",
    "COMMUNITY_CONFIG = {\n",
    "    'resolution': 1.25,  # Can be changed for different resolutions\n",
    "    'test_resolutions': np.arange(0.5, 2.01, 0.05).round(2).tolist()  # For testing multiple resolutions\n",
    "}\n",
    "\n",
    "resolution_results = []\n",
    "\n",
    "for res in COMMUNITY_CONFIG['test_resolutions']:\n",
    "    print(f\"\\n>>> Testing community resolution: {res}\")\n",
    "    results = test_single_configuration(FEATURE_CONFIG, MODEL_CONFIG, CV_CONFIG, community_resolution=res)\n",
    "    \n",
    "    # Store results\n",
    "    resolution_results.append({\n",
    "        'resolution': res,\n",
    "        'mean_auc': results['mean_auc'],\n",
    "        'std_auc': results['std_auc'],\n",
    "        'mean_acc': results['mean_acc'],\n",
    "        'std_acc': results['std_acc']\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "resolution_df = pd.DataFrame(resolution_results)\n",
    "print(\"\\n=== Summary of Results by Resolution ===\")\n",
    "print(resolution_df)\n",
    "\n",
    "# Find best resolution\n",
    "best_row = resolution_df.loc[resolution_df['mean_auc'].idxmax()]\n",
    "print(f\"\\nBest community resolution based on AUC: {best_row['resolution']}\")\n",
    "print(f\"Mean AUC: {best_row['mean_auc']:.4f}, Mean Accuracy: {best_row['mean_acc']:.4f}\")\n",
    "best_row_acc = resolution_df.loc[resolution_df['mean_acc'].idxmax()]\n",
    "print(f\"\\nBest community resolution based on ACC: {best_row['resolution']}\")\n",
    "print(f\"Mean AUC: {best_row_acc['mean_auc']:.4f}, Mean Accuracy: {best_row_acc['mean_acc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e533f0-6ab9-4567-a50a-5b4c99096e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Gridsearch and Randomsearch =====\n",
    "#%% Gridsearch randomforest\n",
    "# ===== GRID SEARCH CONFIGURATION =====\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 300, 400, 500],\n",
    "    'max_depth': [3, 5, 8],\n",
    "    'min_samples_split': [5],\n",
    "    'min_samples_leaf': [6]\n",
    "}\n",
    "\n",
    "best_auc = -1\n",
    "best_params = None\n",
    "grid_results = []\n",
    "\n",
    "# Assume you found this from your resolution tuning\n",
    "best_resolution = 1.25  # Example — replace with your actual best one\n",
    "\n",
    "# Create all combinations of parameters\n",
    "param_combinations = list(product(\n",
    "    param_grid['n_estimators'],\n",
    "    param_grid['max_depth'],\n",
    "    param_grid['min_samples_split'],\n",
    "    param_grid['min_samples_leaf']\n",
    "))\n",
    "\n",
    "print(f\"\\n Starting Grid Search over {len(param_combinations)} configurations...\")\n",
    "\n",
    "for (n_estimators, max_depth, min_samples_split, min_samples_leaf) in param_combinations:\n",
    "    current_config = {\n",
    "        'n_estimators': n_estimators,\n",
    "        'max_depth': max_depth,\n",
    "        'min_samples_split': min_samples_split,\n",
    "        'min_samples_leaf': min_samples_leaf,\n",
    "        'random_state': seed,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n>>> Testing config: {current_config}\")\n",
    "    results = test_single_configuration(FEATURE_CONFIG, current_config, CV_CONFIG, community_resolution=best_resolution)\n",
    "    \n",
    "    grid_results.append({\n",
    "        'n_estimators': n_estimators,\n",
    "        'max_depth': max_depth,\n",
    "        'min_samples_split': min_samples_split,\n",
    "        'min_samples_leaf': min_samples_leaf,\n",
    "        'mean_auc': results['mean_auc'],\n",
    "        'mean_acc': results['mean_acc']\n",
    "    })\n",
    "    \n",
    "    # Track best\n",
    "    if results['mean_auc'] > best_auc:\n",
    "        best_auc = results['mean_auc']\n",
    "        best_params = current_config\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "grid_df = pd.DataFrame(grid_results)\n",
    "grid_df = grid_df.sort_values(by='mean_auc', ascending=False)\n",
    "\n",
    "print(\"\\n=== GRID SEARCH RESULTS ===\")\n",
    "print(grid_df)\n",
    "print(f\"\\n Best Params (by AUC): {best_params}\")\n",
    "print(f\"Mean AUC: {best_auc:.4f}\")\n",
    "\n",
    "#%% Random search\n",
    "import random\n",
    "\n",
    "# ===== RANDOM SEARCH CONFIGURATION =====\n",
    "random_search_space = {\n",
    "    'n_estimators': [50, 100, 200, 300, 400, 500],\n",
    "    'max_depth': [3, 5, 8],\n",
    "    'min_samples_split': [10],\n",
    "    'min_samples_leaf': [6],\n",
    "}\n",
    "\n",
    "# Number of random combinations to test\n",
    "N_RANDOM_TRIALS = 25\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "# Generate all possible combinations, then sample randomly\n",
    "all_combinations = list(product(\n",
    "    random_search_space['n_estimators'],\n",
    "    random_search_space['max_depth'],\n",
    "    random_search_space['min_samples_split'],\n",
    "    random_search_space['min_samples_leaf']\n",
    "))\n",
    "\n",
    "random.seed(seed)\n",
    "sampled_combinations = random.sample(all_combinations, N_RANDOM_TRIALS)\n",
    "\n",
    "best_auc = -1\n",
    "best_params = None\n",
    "random_results = []\n",
    "\n",
    "# Replace with your best resolution found earlier\n",
    "best_resolution = 1.25  # example — replace this!\n",
    "\n",
    "print(f\"\\nStarting Random Search with {N_RANDOM_TRIALS} trials...\")\n",
    "\n",
    "for idx, (n_estimators, max_depth, min_samples_split, min_samples_leaf) in enumerate(sampled_combinations, 1):\n",
    "    current_config = {\n",
    "        'n_estimators': n_estimators,\n",
    "        'max_depth': max_depth,\n",
    "        'min_samples_split': min_samples_split,\n",
    "        'min_samples_leaf': min_samples_leaf,\n",
    "        'random_state': seed,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "\n",
    "    print(f\"\\nTrial {idx}/{N_RANDOM_TRIALS} | Config: {current_config}\")\n",
    "    results = test_single_configuration(FEATURE_CONFIG, current_config, CV_CONFIG, community_resolution=best_resolution)\n",
    "\n",
    "    random_results.append({\n",
    "        'trial': idx,\n",
    "        'n_estimators': n_estimators,\n",
    "        'max_depth': max_depth,\n",
    "        'min_samples_split': min_samples_split,\n",
    "        'min_samples_leaf': min_samples_leaf,\n",
    "        'mean_auc': results['mean_auc'],\n",
    "        'mean_acc': results['mean_acc']\n",
    "    })\n",
    "\n",
    "    if results['mean_auc'] > best_auc:\n",
    "        best_auc = results['mean_auc']\n",
    "        best_params = current_config\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "random_df = pd.DataFrame(random_results)\n",
    "random_df = random_df.sort_values(by='mean_auc', ascending=False)\n",
    "\n",
    "print(\"\\n=== RANDOM SEARCH RESULTS ===\")\n",
    "print(random_df)\n",
    "print(f\"\\nBest Random Search Params (by AUC): {best_params}\")\n",
    "print(f\"Mean AUC: {best_auc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6efdb4c9-0f4a-49aa-99bb-7c368bd0261a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom configuration - modify individual features as needed\n",
    "custom_config = {\n",
    "    # === ATTRIBUTE FEATURES ===\n",
    "    'attribute_i': False,              # Individual attribute value for node i\n",
    "    'attribute_j': False,               # Individual attribute value for node j\n",
    "    'attribute_equality': True,       # Disable attribute equality\n",
    "    \n",
    "    # === BASIC NODE FEATURES ===\n",
    "    'degree_i': False,                  # Degree of node i\n",
    "    'degree_j': False,                  # Degree of node j\n",
    "    'clustering_i': False,             # Disable clustering for node i\n",
    "    'clustering_j': False,             # Disable clustering for node j\n",
    "    'pagerank_i': False,                # PageRank of node i\n",
    "    'pagerank_j': False,                # PageRank of node j\n",
    "    \n",
    "    # === NEIGHBORHOOD FEATURES ===\n",
    "    'common_neighbors': False,          # Number of common neighbors\n",
    "    'jaccard_coefficient': False,       # Jaccard coefficient\n",
    "    'adamic_adar': False,              # Disable Adamic-Adar\n",
    "    'resource_allocation': False,        # Resource Allocation index\n",
    "    'preferential_attachment': False,   # Preferential Attachment\n",
    "    'salton_index': False,             # Disable Salton index\n",
    "    'sorensen_index': False,             # Sorensen index\n",
    "    'two_hop_neighbors': True,        # Disable 2-hop neighbors\n",
    "    \n",
    "    # === DERIVED FEATURES ===\n",
    "    'degree_sum': False,                # Sum of degrees\n",
    "    'degree_diff': True,               # Absolute difference of degrees\n",
    "    'degree_product': False,             # Product of degrees\n",
    "    'degree_ratio': False,              # Ratio of degrees\n",
    "    'clustering_avg': False,           # Disable average clustering\n",
    "    'pagerank_avg': False,             # Disable average PageRank\n",
    "    \n",
    "    # === COMMUNITY FEATURES ===\n",
    "    'same_community': True,             # Same community indicator\n",
    "    'community_size_i': False,           # Community size of node i\n",
    "    'community_size_j': False,           # Community size of node j\n",
    "    'community_size_diff': False,        # Absolute difference in community sizes\n",
    "    'community_size_ratio': False,     # Disable community size ratio\n",
    "}\n",
    "\n",
    "custom_model_config = {\n",
    "    'n_estimators': 50,\n",
    "    'max_depth': 5,\n",
    "    'min_samples_split': 5,\n",
    "    'min_samples_leaf': 6,\n",
    "    'random_state': seed,\n",
    "    'n_jobs': -1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af61e666-2c4a-4276-a60b-4ae9a90fd721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for best configuration\n",
    "def generate_predictions(best_config, best_model_config=MODEL_CONFIG, output_filename='predictions_best_config.csv'):\n",
    "    \"\"\"\n",
    "    Generate predictions using the best individual feature configuration\n",
    "    NOTE: This function trains on ALL available training data (no cross-validation)\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Generating Predictions with Best Configuration ===\")\n",
    "    \n",
    "    # Load test data\n",
    "    inpTest = pd.read_csv('./../assignment2_files_2025/solutionInput.csv', sep=',', index_col='ID')\n",
    "    \n",
    "    # Use ALL training edges for final model (this is the correct approach for Kaggle)\n",
    "    edges = np.array(list(G.edges()))\n",
    "    pos_edges = edges\n",
    "    rng = np.random.default_rng(seed)\n",
    "    non_edges = np.array(list(nx.non_edges(G)))\n",
    "    neg_edges = non_edges[rng.choice(len(non_edges), size=len(pos_edges), replace=False)]\n",
    "    \n",
    "    # Pre-compute metrics on the FULL training graph (this is correct for final submission)\n",
    "    precomputed_metrics = precompute_metrics(G, COMMUNITY_CONFIG['resolution'])\n",
    "    \n",
    "    # Generate features for training data\n",
    "    X_train = []\n",
    "    for (u, v) in np.vstack([pos_edges, neg_edges]):\n",
    "        feat, _ = get_features(G, u, v, best_config, precomputed_metrics)\n",
    "        X_train.append(feat)\n",
    "    \n",
    "    y_train = [1]*len(pos_edges) + [0]*len(neg_edges)\n",
    "    \n",
    "    # Train model\n",
    "    clf = RandomForestClassifier(**best_model_config)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Generate features for test data (using same precomputed metrics)\n",
    "    test_features = []\n",
    "    for _, row in inpTest.iterrows():\n",
    "        feat, _ = get_features(G, int(row.iloc[0]), int(row.iloc[1]), best_config, precomputed_metrics)\n",
    "        test_features.append(feat)\n",
    "    \n",
    "    # Generate predictions\n",
    "    predictions = clf.predict(test_features)\n",
    "    \n",
    "    # Save predictions\n",
    "    sub = pd.DataFrame({'ID': inpTest.index, 'prediction': predictions})\n",
    "    sub.to_csv(output_filename, index=False)\n",
    "    \n",
    "    print(f\"Predictions saved to {output_filename}\")\n",
    "    return clf, test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eabed3c4-9a96-4ba6-ba89-ee9d4f4633d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Generating Predictions with Best Configuration ===\n",
      "Predictions saved to predictions_final2.csv\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions with current configuration\n",
    "clf, test_features = generate_predictions(custom_config, best_model_config = custom_model_config, output_filename ='predictions_final2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6244ff-8e79-47cf-bc19-38951b43dee8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f9b2216-961e-4d9e-a000-31f3247045a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\schud\\anaconda3\\envs\\lab5\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './../assignment2_files_2025/edges_train.edgelist'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Use features:\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# ===== Load & visualize graph =====\u001b[39;00m\n\u001b[0;32m     22\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./../assignment2_files_2025/edges_train.edgelist\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 23\u001b[0m G \u001b[38;5;241m=\u001b[39m \u001b[43mnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_edgelist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnodetype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_using\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGraph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGraph loaded with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mG\u001b[38;5;241m.\u001b[39mnumber_of_nodes()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m nodes and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mG\u001b[38;5;241m.\u001b[39mnumber_of_edges()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m edges\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Attributes\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\lab5\\Lib\\site-packages\\networkx\\utils\\decorators.py:784\u001b[0m, in \u001b[0;36margmap.__call__.<locals>.func\u001b[1;34m(_argmap__wrapper, *args, **kwargs)\u001b[0m\n\u001b[0;32m    783\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfunc\u001b[39m(\u001b[38;5;241m*\u001b[39margs, __wrapper\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43margmap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lazy_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m__wrapper\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<class 'networkx.utils.decorators.argmap'> compilation 6:3\u001b[0m, in \u001b[0;36margmap_read_edgelist_1\u001b[1;34m(path, comments, delimiter, create_using, nodetype, data, edgetype, encoding, backend, **backend_kwargs)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbz2\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcollections\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgzip\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mitertools\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\lab5\\Lib\\site-packages\\networkx\\utils\\decorators.py:194\u001b[0m, in \u001b[0;36mopen_file.<locals>._open_file\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;66;03m# could be None, or a file handle, in which case the algorithm will deal with it\u001b[39;00m\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m path, \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 194\u001b[0m fobj \u001b[38;5;241m=\u001b[39m \u001b[43m_dispatch_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fobj, \u001b[38;5;28;01mlambda\u001b[39;00m: fobj\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './../assignment2_files_2025/edges_train.edgelist'"
     ]
    }
   ],
   "source": [
    "# ===== Imports =====\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Define random seed\n",
    "seed = 42\n",
    "\n",
    "# Use features:\n",
    "# ===== Load & visualize graph =====\n",
    "path = \"./../assignment2_files_2025/edges_train.edgelist\"\n",
    "G = nx.read_edgelist(path, delimiter=',', nodetype=int, create_using=nx.Graph())\n",
    "\n",
    "print(f\"Graph loaded with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n",
    "\n",
    "# Attributes\n",
    "pathattributes = \"./../assignment2_files_2025/attributes.csv\"\n",
    "attributes_df = pd.read_csv(pathattributes)\n",
    "node_id_col = attributes_df.columns[0]\n",
    "attribute_cols = [col for col in attributes_df.columns if col != node_id_col]\n",
    "\n",
    "# ===== Feature engineering =====\n",
    "def get_features(G, i, j):\n",
    "    \"\"\"\n",
    "    Features voor node pair (i, j)\n",
    "    Totaal: 4 attribute + 20 graph features = 24 features\n",
    "    \"\"\"\n",
    "    features = []\n",
    "\n",
    "    attrs_i = attributes_dict.get(i, {})\n",
    "    attrs_j = attributes_dict.get(j, {})\n",
    "\n",
    "    for col in attribute_cols:\n",
    "        val_i = attrs_i.get(col, 0)\n",
    "        val_j = attrs_j.get(col, 0)\n",
    "        features.append(val_i)\n",
    "        features.append(val_j)\n",
    "        #features.append(int(val_i != val_j))        # verschil\n",
    "        features.append(int(val_i == val_j))\n",
    "\n",
    "    # --- Basic node features (6) ---\n",
    "    deg_i = G.degree(i)\n",
    "    deg_j = G.degree(j)\n",
    "    cc_i = clustering.get(i, 0)\n",
    "    cc_j = clustering.get(j, 0)\n",
    "    pr_i = pagerank.get(i, 0)\n",
    "    pr_j = pagerank.get(j, 0)\n",
    "\n",
    "    # --- Neighborhood features (7) ---\n",
    "    common = list(nx.common_neighbors(G, i, j))\n",
    "    cn_ij = len(common)\n",
    "\n",
    "    neigh_i = set(G.neighbors(i))\n",
    "    neigh_j = set(G.neighbors(j))\n",
    "    union_sz = len(neigh_i | neigh_j)\n",
    "    \n",
    "    # Jaccard\n",
    "    jc_ij = next(nx.jaccard_coefficient(G, [(i,j)]))[2]\n",
    "    \n",
    "    # Adamic-Adar\n",
    "    aa_ij = sum(1.0 / np.log(G.degree(z)) for z in common if G.degree(z) > 1)\n",
    "\n",
    "    # Resource Allocation\n",
    "    ra_ij = sum(1.0 / G.degree(z) for z in common if G.degree(z) > 0)\n",
    "    \n",
    "    # Preferential Attachment\n",
    "    pa_ij = pa[i, j]\n",
    "\n",
    "    # Salton\n",
    "    salton = cn_ij / np.sqrt(deg_i * deg_j) if (deg_i * deg_j) > 0 else 0\n",
    "    \n",
    "    # Sorensen\n",
    "    sorensen = (2 * cn_ij) / (deg_i + deg_j) if (deg_i + deg_j) > 0 else 0\n",
    "\n",
    "    #Katz:\n",
    "    katz_ij = katz_matrix[i, j]\n",
    "\n",
    "    # 2 hop neighbors\n",
    "    # nodes reachable within 2 hops from i\n",
    "    dist2_i = set(nx.single_source_shortest_path_length(G, i, cutoff=2))\n",
    "    # nodes reachable within 2 hops from j\n",
    "    dist2_j = set(nx.single_source_shortest_path_length(G, j, cutoff=2))\n",
    "    \n",
    "    # remove the nodes themselves and their direct neighbors\n",
    "    dist2_i -= {i} | set(G.neighbors(i))\n",
    "    dist2_j -= {j} | set(G.neighbors(j))\n",
    "    \n",
    "    # intersection of these “exactly distance-2” neighborhoods\n",
    "    dist2_count = len(dist2_i & dist2_j)\n",
    "\n",
    "    # --- Derived features (6) ---\n",
    "    deg_sum = deg_i + deg_j\n",
    "    deg_diff = abs(deg_i - deg_j)\n",
    "    deg_product = deg_i * deg_j\n",
    "    deg_ratio = min(deg_i, deg_j) / max(deg_i, deg_j) if max(deg_i, deg_j) > 0 else 0\n",
    "    cc_avg = (cc_i + cc_j) / 2\n",
    "    pr_avg = (pr_i + pr_j) / 2\n",
    "\n",
    "    # --- Communties ---\n",
    "    # --- Community features ---\n",
    "    comm_i = community_dict.get(i, -1)\n",
    "    comm_j = community_dict.get(j, -1)\n",
    "    \n",
    "    # Same community indicator\n",
    "    same_comm = int(comm_i == comm_j)\n",
    "    # Comm size\n",
    "    size_i = comm_sizes.get(comm_i, 0)\n",
    "    size_j = comm_sizes.get(comm_j, 0)\n",
    "    # Abs comm size diff\n",
    "    abs_size_diff = abs(size_i - size_j)\n",
    "    # Relative comm size diff\n",
    "    if size_i + size_j > 0:\n",
    "        rel_size_ratio = size_i / (size_j + 1e-6)\n",
    "    else:\n",
    "        rel_size_ratio = 0\n",
    "\n",
    "    # Combine all features\n",
    "    features.extend([\n",
    "        cc_i, cc_j,\n",
    "        aa_ij, ra_ij, pa_ij,\n",
    "        deg_product, \n",
    "    abs_size_diff, \n",
    "    katz_ij\n",
    "    ])\n",
    "    \n",
    "\n",
    "    return np.array(features, dtype=float)\n",
    "\n",
    "# ===== Kfold splitting for testing =====\n",
    "edges = np.array(list(G.edges()))\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "auc_scores = []\n",
    "acc_scores = []\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(edges), 1):\n",
    "    print(f\"\\n=== Fold {fold} ===\")\n",
    "\n",
    "    # Split edges\n",
    "    train_edges = edges[train_idx]\n",
    "    val_edges   = edges[val_idx]\n",
    "\n",
    "    # Build training graph\n",
    "    G_train = nx.Graph()\n",
    "    G_train.add_nodes_from(G.nodes())\n",
    "    G_train.add_edges_from(train_edges)\n",
    "\n",
    "    # ===== PRE-COMPUTE METRICS =====\n",
    "    print(f\"\\n2.{fold} Pre-computing graph metrics...\")\n",
    "    \n",
    "    N = G_train.number_of_nodes()\n",
    "    \n",
    "    # Preferential Attachment\n",
    "    pa = np.zeros((N, N))\n",
    "    for u, v, p in nx.preferential_attachment(G_train, [(i, j) for i in range(N) for j in range(N)]):\n",
    "        pa[u, v] = p\n",
    "    \n",
    "    # PageRank\n",
    "    pagerank = nx.pagerank(G_train)\n",
    "    \n",
    "    # Clustering        \n",
    "    clustering = nx.clustering(G_train)\n",
    "\n",
    "    # Communities\n",
    "    communities = list(nx.algorithms.community.greedy_modularity_communities(G_train, resolution=1))\n",
    "    community_dict = {n: cid for cid, nodes in enumerate(communities) for n in nodes}\n",
    "    comm_sizes = {cid: len(nodes) for cid, nodes in enumerate(communities)}\n",
    "    print(\"   ✓ Metrics computed\")\n",
    "\n",
    "    # Katz\n",
    "    # Create adjacency matrix\n",
    "    A = nx.to_numpy_array(G_train)\n",
    "    beta = 0.01\n",
    "    I = np.eye(A.shape[0])\n",
    "    \n",
    "    # Katz matrix: (I - beta*A)^-1 - I\n",
    "    katz_matrix = np.linalg.inv(I - beta * A) - I\n",
    "\n",
    "    # Normalize\n",
    "    katz_values = katz_matrix.flatten().reshape(-1, 1)\n",
    "    katz_matrix = MinMaxScaler().fit_transform(katz_values).reshape(A.shape)\n",
    "\n",
    "    # Build training data\n",
    "    pos_train = train_edges\n",
    "    rng = np.random.default_rng(seed)\n",
    "    non_edges = np.array(list(nx.non_edges(G_train)))\n",
    "    neg_train = non_edges[rng.choice(len(non_edges), size=len(pos_train), replace=False)]\n",
    "    X_train = [get_features(G_train, u, v) for (u,v) in np.vstack([pos_train, neg_train])]\n",
    "    y_train = [1]*len(pos_train) + [0]*len(neg_train)\n",
    "\n",
    "    # Build validation data (features from G_train!)\n",
    "    pos_val = val_edges\n",
    "    rng2 = np.random.default_rng(seed+1)\n",
    "    non_edges = np.array(list(nx.non_edges(G_train)))\n",
    "    neg_val = non_edges[rng2.choice(len(non_edges), size=len(pos_val), replace=False)]\n",
    "    X_val = [get_features(G_train, u, v) for (u,v) in np.vstack([pos_val, neg_val])]\n",
    "    y_val = [1]*len(pos_val) + [0]*len(neg_val)\n",
    "\n",
    "    # Train + evaluate\n",
    "    # Model configuratie\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=200,       # Aantal trees\n",
    "        max_depth=2,           # Minder diep (was 12)\n",
    "        min_samples_split=12,   # Meer samples nodig (was 8)\n",
    "        min_samples_leaf=5,     # Grotere leafs (was 4)\n",
    "        max_features='sqrt',    # Features per split\n",
    "        random_state=42,\n",
    "        n_jobs=-1              # Gebruik alle CPU cores\n",
    "    )\n",
    "\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict_proba(X_val)[:,1]\n",
    "    auc = roc_auc_score(y_val, y_pred)\n",
    "    auc_scores.append(auc)\n",
    "    \n",
    "    # Predict on validation set\n",
    "    y_pred_val = clf.predict(X_val)  # use .predict, not .predict_proba\n",
    "    \n",
    "    # Compute accuracy\n",
    "    val_acc = accuracy_score(y_val, y_pred_val)\n",
    "    acc_scores.append(val_acc)\n",
    "    \n",
    "    print(f\"Fold {fold} AUC: {auc:.4f} Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "    importances = clf.feature_importances_\n",
    "\n",
    "# Get feature names\n",
    "attribute_pairs = [f\"{col}_i\" for col in attribute_cols] + [f\"{col}_j\" for col in attribute_cols] + [f\"{col}_eq\" for col in attribute_cols]\n",
    "\n",
    "graph_features = [\n",
    "        'cc_i', 'cc_j',\n",
    "        'aa_ij', 'ra_ij', 'pa_ij',\n",
    "        'deg_product', \n",
    "    'abs_size_diff', \n",
    "    'katz_ij'\n",
    "    ]\n",
    "\n",
    "feature_names = graph_features + attribute_pairs\n",
    "# Rank features\n",
    "feat_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(feat_df['Feature'][:20][::-1], feat_df['Importance'][:20][::-1])\n",
    "plt.title(\"Top 20 Feature Importances (Random Forest)\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.show()\n",
    "\n",
    "X = np.array([get_features(G, u, v) for (u,v) in np.vstack([pos_train, neg_train])])\n",
    "corr = abs(pd.DataFrame(X, columns=feature_names).corr())\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.imshow(corr, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.xticks(ticks=np.arange(len(feature_names)), labels=feature_names, rotation=90)\n",
    "plt.yticks(ticks=np.arange(len(feature_names)), labels=feature_names)\n",
    "plt.colorbar()\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\nMean AUC: {np.mean(auc_scores):.4f} ± {np.std(auc_scores):.4f}\")\n",
    "print(f\"\\nMean Acc: {np.mean(acc_scores):.4f} ± {np.std(acc_scores):.4f}\")\n",
    "\n",
    "# commsizeacc[res] = np.mean(acc_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0eba422-b5b5-41f5-924d-b18b9240225d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

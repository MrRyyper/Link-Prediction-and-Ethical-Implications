{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1900a23",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StratifiedKFold\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mensemble\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"LINK PREDICTION - RANDOM FOREST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ===== STAP 1: DATA LADEN =====\n",
    "print(\"\\n1. Loading data...\")\n",
    "\n",
    "# Graph\n",
    "G = nx.read_edgelist(\"edges_train.edgelist\", delimiter=',', nodetype=int, create_using=nx.Graph())\n",
    "print(f\"   âœ“ Graph: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
    "\n",
    "# Attributes\n",
    "attributes_df = pd.read_csv('attributes.csv')\n",
    "node_id_col = attributes_df.columns[0]\n",
    "attribute_cols = [col for col in attributes_df.columns if col != node_id_col]\n",
    "\n",
    "# Encode categorisch naar numeriek\n",
    "for col in attribute_cols:\n",
    "    if attributes_df[col].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        attributes_df[col] = le.fit_transform(attributes_df[col].astype(str))\n",
    "        print(f\"   âœ“ Encoded '{col}': {list(le.classes_)}\")\n",
    "\n",
    "attributes_dict = attributes_df.set_index(node_id_col).to_dict('index')\n",
    "\n",
    "# Test data\n",
    "solution_input = pd.read_csv('solutionInput.csv', sep=',', index_col='ID')\n",
    "print(f\"   âœ“ Test set: {len(solution_input)} pairs\")\n",
    "\n",
    "# ===== STAP 2: PRE-COMPUTE METRICS =====\n",
    "print(\"\\n2. Pre-computing graph metrics...\")\n",
    "\n",
    "N = G.number_of_nodes()\n",
    "\n",
    "# Preferential Attachment\n",
    "pa = np.zeros((N, N))\n",
    "for u, v, p in nx.preferential_attachment(G, [(i, j) for i in range(N) for j in range(N)]):\n",
    "    pa[u, v] = p\n",
    "\n",
    "# PageRank\n",
    "pagerank = nx.pagerank(G)\n",
    "\n",
    "# Clustering\n",
    "clustering = nx.clustering(G)\n",
    "\n",
    "print(\"   âœ“ Metrics computed\")\n",
    "\n",
    "# ===== STAP 3: FEATURE ENGINEERING =====\n",
    "print(\"\\n3. Feature engineering...\")\n",
    "\n",
    "def getFeature(G, i, j):\n",
    "    \"\"\"\n",
    "    Features voor node pair (i, j)\n",
    "    Totaal: 4 attribute + 20 graph features = 24 features\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # --- Attribute features (4) ---\n",
    "    attrs_i = attributes_dict.get(i, {})\n",
    "    attrs_j = attributes_dict.get(j, {})\n",
    "    \n",
    "    for col in attribute_cols:\n",
    "        val_i = attrs_i.get(col, 0)\n",
    "        val_j = attrs_j.get(col, 0)\n",
    "        features.append(val_i)\n",
    "        features.append(val_j)\n",
    "        features.append(abs(val_i - val_j))        # verschil\n",
    "        features.append(int(val_i == val_j))       # zelfde waarde?\n",
    "    \n",
    "    # --- Basic node features (6) ---\n",
    "    deg_i = G.degree(i)\n",
    "    deg_j = G.degree(j)\n",
    "    cc_i = clustering.get(i, 0)\n",
    "    cc_j = clustering.get(j, 0)\n",
    "    pr_i = pagerank.get(i, 0)\n",
    "    pr_j = pagerank.get(j, 0)\n",
    "    \n",
    "    # --- Neighborhood features (7) ---\n",
    "    common = list(nx.common_neighbors(G, i, j))\n",
    "    cn_ij = len(common)\n",
    "    \n",
    "    neigh_i = set(G.neighbors(i))\n",
    "    neigh_j = set(G.neighbors(j))\n",
    "    union_sz = len(neigh_i | neigh_j)\n",
    "    \n",
    "    # Jaccard\n",
    "    jc_ij = (cn_ij / union_sz) if union_sz > 0 else 0.0\n",
    "    \n",
    "    # Adamic-Adar\n",
    "    aa_ij = sum(1.0 / np.log(G.degree(z)) for z in common if G.degree(z) > 1)\n",
    "    \n",
    "    # Resource Allocation\n",
    "    ra_ij = sum(1.0 / G.degree(z) for z in common if G.degree(z) > 0)\n",
    "    \n",
    "    # Preferential Attachment\n",
    "    pa_ij = pa[i, j]\n",
    "    \n",
    "    # Salton\n",
    "    salton = cn_ij / np.sqrt(deg_i * deg_j) if (deg_i * deg_j) > 0 else 0\n",
    "    \n",
    "    # Sorensen\n",
    "    sorensen = (2 * cn_ij) / (deg_i + deg_j) if (deg_i + deg_j) > 0 else 0\n",
    "    \n",
    "    # --- Distance feature (1) ---\n",
    "    # Aantal gemeenschappelijke neighbors op distance 2\n",
    "    dist2_count = len([n for n in G.neighbors(i) if j in G.neighbors(n)])\n",
    "    \n",
    "    # --- Derived features (6) ---\n",
    "    deg_sum = deg_i + deg_j\n",
    "    deg_diff = abs(deg_i - deg_j)\n",
    "    deg_product = deg_i * deg_j\n",
    "    deg_ratio = min(deg_i, deg_j) / max(deg_i, deg_j) if max(deg_i, deg_j) > 0 else 0\n",
    "    cc_avg = (cc_i + cc_j) / 2\n",
    "    pr_avg = (pr_i + pr_j) / 2\n",
    "    \n",
    "    # Combine all features\n",
    "    features.extend([\n",
    "        deg_i, deg_j, cc_i, cc_j, pr_i, pr_j,\n",
    "        cn_ij, aa_ij, jc_ij, ra_ij, pa_ij, salton, sorensen,\n",
    "        dist2_count,\n",
    "        deg_sum, deg_diff, deg_product, deg_ratio, cc_avg, pr_avg\n",
    "    ])\n",
    "    \n",
    "    return np.array(features, dtype=float)\n",
    "\n",
    "print(f\"   âœ“ Features: {len(attribute_cols) * 4} attribute + 20 graph = {len(attribute_cols) * 4 + 20} total\")\n",
    "\n",
    "# ===== STAP 4: CREATE DATASET =====\n",
    "print(\"\\n4. Creating dataset...\")\n",
    "\n",
    "X, Y = [], []\n",
    "\n",
    "# Positive samples (existing edges)\n",
    "for i, j in G.edges():\n",
    "    X.append(getFeature(G, i, j))\n",
    "    Y.append(1)\n",
    "\n",
    "# Negative samples (random non-edges, 1:1 ratio)\n",
    "pos_count = len(G.edges())\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "neg_added = 0\n",
    "while neg_added < pos_count:\n",
    "    i = int(rng.integers(0, N))\n",
    "    j = int(rng.integers(0, N))\n",
    "    if not G.has_edge(i, j) and i != j:\n",
    "        X.append(getFeature(G, i, j))\n",
    "        Y.append(0)\n",
    "        neg_added += 1\n",
    "\n",
    "print(f\"   âœ“ Dataset: {len(X)} samples ({pos_count} positive, {neg_added} negative)\")\n",
    "\n",
    "# ===== STAP 5: TRAIN MODEL MET 5-FOLD CROSS-VALIDATION =====\n",
    "print(\"\\n5. Training Random Forest met 5-Fold Cross-Validation...\")\n",
    "\n",
    "# Model configuratie\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=200,       # Aantal trees\n",
    "    max_depth=10,           # Minder diep (was 12)\n",
    "    min_samples_split=12,   # Meer samples nodig (was 8)\n",
    "    min_samples_leaf=5,     # Grotere leafs (was 4)\n",
    "    max_features='sqrt',    # Features per split\n",
    "    random_state=42,\n",
    "    n_jobs=-1              # Gebruik alle CPU cores\n",
    ")\n",
    "\n",
    "# 5-Fold Cross-Validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = []\n",
    "\n",
    "print(\"\\n   Cross-Validation Results:\")\n",
    "print(\"   \" + \"-\" * 50)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, Y), 1):\n",
    "    # Split data\n",
    "    X_train_fold = [X[i] for i in train_idx]\n",
    "    y_train_fold = [Y[i] for i in train_idx]\n",
    "    X_val_fold = [X[i] for i in val_idx]\n",
    "    y_val_fold = [Y[i] for i in val_idx]\n",
    "    \n",
    "    # Train\n",
    "    clf.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred_val = clf.predict(X_val_fold)\n",
    "    fold_acc = accuracy_score(y_val_fold, y_pred_val)\n",
    "    cv_scores.append(fold_acc)\n",
    "    \n",
    "    print(f\"   Fold {fold}: {fold_acc:.4f}\")\n",
    "\n",
    "cv_mean = np.mean(cv_scores)\n",
    "cv_std = np.std(cv_scores)\n",
    "\n",
    "print(\"   \" + \"-\" * 50)\n",
    "print(f\"   Mean Accuracy:  {cv_mean:.4f}\")\n",
    "print(f\"   Std Dev:        {cv_std:.4f}\")\n",
    "print(f\"   95% CI:         [{cv_mean - 1.96*cv_std:.4f}, {cv_mean + 1.96*cv_std:.4f}]\")\n",
    "\n",
    "# Train final model op ALLE data (voor beste predictions)\n",
    "print(\"\\n   Training final model on all data...\")\n",
    "clf.fit(X, Y)\n",
    "print(\"   âœ“ Training complete\")\n",
    "\n",
    "# ===== STAP 6: FINAL EVALUATION =====\n",
    "print(\"\\n6. Final Evaluation...\")\n",
    "\n",
    "# Evalueer op volledige dataset (om feature importance te krijgen)\n",
    "y_pred_all = clf.predict(X)\n",
    "train_acc = accuracy_score(Y, y_pred_all)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"RESULTS:\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Cross-Validation (5-Fold):\")\n",
    "print(f\"  Mean Accuracy:   {cv_mean:.4f}\")\n",
    "print(f\"  Std Dev:         {cv_std:.4f}\")\n",
    "print(f\"  Expected Range:  [{cv_mean - cv_std:.4f}, {cv_mean + cv_std:.4f}]\")\n",
    "print(f\"\\nFull Training Set:\")\n",
    "print(f\"  Accuracy:        {train_acc:.4f}\")\n",
    "print(f\"\\nðŸ’¡ Verwachte Kaggle score: ~{cv_mean:.4f} (Â±{cv_std:.4f})\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# ===== STAP 7: FEATURE IMPORTANCE =====\n",
    "print(\"\\n7. Feature Importance Analysis...\")\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "feature_names = []\n",
    "for col in attribute_cols:\n",
    "    feature_names.extend([f'{col}_i', f'{col}_j', f'{col}_diff', f'{col}_same'])\n",
    "feature_names.extend([\n",
    "    'deg_i', 'deg_j', 'cc_i', 'cc_j', 'pr_i', 'pr_j',\n",
    "    'cn', 'aa', 'jc', 'ra', 'pa', 'salton', 'sorensen',\n",
    "    'dist2_count',\n",
    "    'deg_sum', 'deg_diff', 'deg_prod', 'deg_ratio', 'cc_avg', 'pr_avg'\n",
    "])\n",
    "\n",
    "importances = clf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "for i in range(min(10, len(feature_names))):\n",
    "    print(f\"  {i+1:2d}. {feature_names[indices[i]]:25s} {importances[indices[i]]:.4f}\")\n",
    "\n",
    "# ===== STAP 8: KAGGLE SUBMISSION =====\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"8. Generating Kaggle submission...\")\n",
    "\n",
    "# Compute features for test set\n",
    "test_features = np.array([\n",
    "    getFeature(G, int(row[0]), int(row[1])) \n",
    "    for _, row in solution_input.iterrows()\n",
    "])\n",
    "\n",
    "# Predictions\n",
    "predictions = clf.predict(test_features)\n",
    "\n",
    "# Save\n",
    "submission = pd.DataFrame({'ID': solution_input.index, 'prediction': predictions})\n",
    "submission.to_csv('prediction_RF.csv', index=False)\n",
    "\n",
    "print(f\"   âœ“ Saved: prediction_RF.csv\")\n",
    "print(f\"   âœ“ Predictions: {sum(predictions)} positive, {len(predictions) - sum(predictions)} negative\")\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"âœ… DONE! Upload 'prediction_RF.csv' to Kaggle\")\n",
    "print(f\"ðŸ’¡ Verwachte Kaggle score: ~{cv_mean:.4f} (Â±{cv_std:.4f})\")\n",
    "print(f\"{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
